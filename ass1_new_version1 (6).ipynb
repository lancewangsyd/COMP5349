{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ass1_new_version1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "MrCs3KZT7vkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### the three blocks are the whole model"
      ]
    },
    {
      "metadata": {
        "id": "8lh3M9wZ4rhf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "import math\n",
        "\n",
        "class Activation(object):\n",
        "\n",
        "      \n",
        "    def __relu(self,x):\n",
        "        \n",
        "        return np.maximum(0,x)\n",
        "      \n",
        "    def __relu_deriv(self,a):\n",
        "        a[a>0]=1\n",
        "        a[a<=0]=0\n",
        "        return a\n",
        "      \n",
        "    def __softmax(self, x):\n",
        "        \"\"\"\n",
        "        x is of shape(m,n_in)\n",
        "        \"\"\"\n",
        "        y= np.exp(x)/np.sum(np.exp(x),axis=1).reshape(-1,1)\n",
        "        assert(y.shape == x.shape)\n",
        "        \n",
        "        return y\n",
        "      \n",
        "    def __softmax_deriv(self, a):\n",
        "        \n",
        "        y = a*(1-a)\n",
        "        assert(y.shape == a.shape)\n",
        "        return y\n",
        "\n",
        "    def __init__(self,activation='relu'):\n",
        "        if activation == 'softmax':\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "skcEQOleWrhj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class HiddenLayer(object):    \n",
        "    def __init__(self, n_in, n_out, W=None, b=None, activation = \"relu\" ):\n",
        "        \n",
        "        self.act = activation\n",
        "\n",
        "        self.input=None\n",
        "        self.activation = Activation(activation).f\n",
        "        self.activation_deriv = Activation(activation).f_deriv\n",
        "        # end-snippet-1\n",
        "\n",
        "        # `W` is initialized with `W_values` which is uniformely sampled\n",
        "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
        "        # Note : optimal initialization of weights is dependent on the\n",
        "        #        activation function used (among other things).\n",
        "        #        For example, results presented in [Xavier10] suggest that you\n",
        "        #        should use 4 times larger initial weights for sigmoid\n",
        "        #        compared to tanh\n",
        "        #        We have no info for other function, so we use the same as\n",
        "        #        tanh.\n",
        "        np.random.seed(7)\n",
        "        \n",
        "        self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                high=np.sqrt(6. / (n_in + n_out)),\n",
        "                size=(n_in, n_out)\n",
        "        )\n",
        "        \n",
        "        self.b = np.zeros((1, n_out))\n",
        "        \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        self.v_dW = np.zeros(self.W.shape)\n",
        "        self.v_db = np.zeros(self.b.shape)\n",
        "        \n",
        "    def forward(self, input, keep_prob):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :input.shape = (m, n_in), b.shape = (1, n_out), W.shape = (n_in, n_out)\n",
        "        :lin_output.shape = (m, n_out)\n",
        "        '''\n",
        "\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        \n",
        "        assert(self.W.shape[1] == self.b.shape[1])\n",
        "        assert(input.shape[1] == self.W.shape[0])\n",
        "        assert(input.shape[0] == lin_output.shape[0])\n",
        "        \n",
        "        if self.act == \"softmax\":\n",
        "            keep_prob = 1\n",
        "  \n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "\n",
        "        output = np.atleast_2d(self.output)\n",
        "\n",
        "        d = np.random.rand(output.shape[0],output.shape[1]) < keep_prob\n",
        "        output = np.multiply(d, output)\n",
        "        output /= keep_prob\n",
        "        \n",
        "        assert(output.shape == self.output.shape)\n",
        "\n",
        "        self.input=input\n",
        "        self.output = output\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, beta, keep_prob):       \n",
        "        \"\"\"\n",
        "        :self.input.shape = (m, n_in)\n",
        "        :delta.shape = (m, n_out)\n",
        "        :self.grad_W.shape = (n_in, n_out)\n",
        "        :self.grad_b.shape = (1, n_out)\n",
        "        :delta_.shape = (m, n_in)\n",
        "        \"\"\"\n",
        "        delta = np.atleast_2d(delta)\n",
        "        self.input = np.atleast_2d(self.input)\n",
        "        assert(self.input.shape[0] == delta.shape[0])\n",
        "        m = self.input.shape[0]\n",
        "        \n",
        "        self.grad_W = self.input.T.dot(delta)/m\n",
        "        self.grad_b = np.sum(delta, axis = 0)/m\n",
        "        \n",
        "        self.grad_b = np.atleast_2d(self.grad_b)\n",
        "        \n",
        "\n",
        "        self.v_dW = beta * self.v_dW + (1-beta) * self.grad_W\n",
        "        self.v_db = beta * self.v_db + (1-beta) * self.grad_b\n",
        "\n",
        "        assert(self.v_dW.shape == self.grad_W.shape)\n",
        "        assert(self.v_db.shape == self.grad_b.shape)\n",
        "        \n",
        "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        delta_ /= keep_prob\n",
        "        assert(delta_.shape == self.input.shape)\n",
        "        return delta_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GjkBweoHBh02",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "\n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "\n",
        "        self.layers=[]\n",
        "        self.layers_number = layers\n",
        "        self.params=[]\n",
        "\n",
        "        self.activation= None \n",
        "        \n",
        "        for i in range(len(layers)-2):\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1], activation=\"relu\"))\n",
        "\n",
        "        L = len(layers)\n",
        "        self.layers.append(HiddenLayer(layers[L-2],layers[L-1], activation=\"softmax\"))\n",
        "            \n",
        "    def forward(self, input, keep_prob):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input, keep_prob)\n",
        "            input=output\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def criterion_cross_entropy(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        y.shape = y-hat.shape = (m, yn)\n",
        "        delta.shape = (m, yn)\n",
        "        \"\"\"\n",
        "      \n",
        "        assert(y.shape == y_hat.shape)\n",
        "        \n",
        "        activation_deriv=Activation(\"softmax\").f_deriv\n",
        "        \n",
        "        loss = -np.sum(np.log(y_hat) * y) \n",
        "                \n",
        "        error = y-y_hat\n",
        "        \n",
        "        delta = error*activation_deriv(y_hat)  \n",
        "        assert(delta.shape == y.shape)\n",
        "        return loss, delta\n",
        "      \n",
        "    \n",
        "    def backward(self,delta, beta, keep_prob):        \n",
        "        for layer in reversed(self.layers):\n",
        "            delta=layer.backward(delta, beta, keep_prob)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        \"\"\"\n",
        "        v_db.shape = (m, n_out)\n",
        "        b.shape = (1, n_out)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            layer.W += lr * layer.v_dW                   \n",
        "            layer.b += lr * layer.v_db\n",
        "            \n",
        "   \n",
        "    def create_mini_batch(self, mini_batch_size, labels, X, y, yn, num_features, m):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        #initializations\n",
        "        np.random.seed(1)\n",
        "\n",
        "        mini_batches = []            \n",
        "        permutation = list(np.random.permutation(m))\n",
        "        shuffled_X = X[permutation, :]\n",
        "        shuffled_y = y[permutation, :]\n",
        "        \n",
        "        labels = labels[permutation]\n",
        "        mini_batches_labels = labels\n",
        "\n",
        "        num_com_batches = math.floor(m/mini_batch_size)\n",
        "        \n",
        "        for k in range(0, num_com_batches):\n",
        "            mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)* mini_batch_size, : ]\n",
        "            mini_batch_y = shuffled_y[k*mini_batch_size:(k+1)* mini_batch_size, : ]\n",
        "            mini_batch = (mini_batch_X, mini_batch_y)\n",
        "            mini_batches.append(mini_batch)\n",
        "            assert(mini_batch_X.shape == (mini_batch_size, num_features))\n",
        "            assert(mini_batch_y.shape == (mini_batch_size, yn))\n",
        "            \n",
        "        if m % mini_batch_size !=0: \n",
        "            mini_batch_X = shuffled_X[(k+1)*mini_batch_size:, : ]\n",
        "            mini_batch_y = shuffled_y[(k+1)*mini_batch_size:, : ]\n",
        "            mini_batch = (mini_batch_X, mini_batch_y)\n",
        "            mini_batches.append(mini_batch)\n",
        "            \n",
        "        return mini_batches, mini_batches_labels\n",
        "      \n",
        "    def fit_pre_processing(self, X, labels, epochs):\n",
        "      \n",
        "        labels = labels.flatten()\n",
        "        assert(X.shape[0] == labels.shape[0])\n",
        "\n",
        "        y = (np.arange(self.layers_number[-1]) == labels[:, None]).astype(float)\n",
        "        yn = y.shape[1]\n",
        "        assert(y.shape[0] == X.shape[0])\n",
        "                \n",
        "        num_features = X.shape[1]       \n",
        "        m = X.shape[0]\n",
        "        \n",
        "        to_return_cost = np.zeros(epochs)\n",
        "        to_return_accuracy = np.zeros(epochs)\n",
        "        \n",
        "        return y, yn, num_features, m, to_return_cost, to_return_accuracy\n",
        "      \n",
        "    def fit(self, X, labels, learning_rate=0.1, epochs=100, keep_prob = 1, beta=0, mini_batch_size = 1, X_test = None, labels_test = None, ):\n",
        "        \n",
        "        \"\"\"\n",
        "        :X.shape = (m, num_features)\n",
        "        :labels.shape = (m, 1) / (m,)\n",
        "        :y.shape = (m, yn)\n",
        "        :y_hat.shape = y.shape\n",
        "        \"\"\"\n",
        "        y, yn, num_features, m, to_return_cost, to_return_accuracy = self.fit_pre_processing(X, labels, epochs)\n",
        "        \n",
        "        if X_test and labels_test:\n",
        "            y_test, yn_test, num_features_test, m_test, to_return_cost_test, to_return_accuracy_test = self.fit_pre_processing(X_test, labels_test, epochs)\n",
        "\n",
        "            assert(num_features == num_features_test)\n",
        "            assert(yn == yn_test)\n",
        "\n",
        "            num_features_test = num_features\n",
        "            yn_test = yn\n",
        "\n",
        "        mini_batches, mini_batches_labels = self.create_mini_batch(mini_batch_size, labels, X, y, yn, num_features, m)\n",
        "\n",
        "        for k in range(epochs):\n",
        "            y_hat_labels = []\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                \n",
        "                mini_batch_X, mini_batch_y = mini_batch\n",
        "                \n",
        "                mini_batch_y_hat = self.forward(mini_batch_X, keep_prob)\n",
        "                \n",
        "                loss, delta=self.criterion_cross_entropy(mini_batch_y, mini_batch_y_hat)\n",
        "                \n",
        "                self.backward(delta, beta, keep_prob)\n",
        "                \n",
        "                self.update(learning_rate)\n",
        "                \n",
        "                y_hat_label = mini_batch_y_hat.argmax(axis = 1)\n",
        "                \n",
        "                y_hat_labels = np.append(y_hat_labels, y_hat_label)\n",
        "    \n",
        "            to_return_cost[k] = np.mean(loss)\n",
        "            \n",
        "            assert(len(y_hat_labels) == len(mini_batches_labels))\n",
        "            to_return_accuracy[k] = str(np.sum((y_hat_labels == mini_batches_labels)/m)) \n",
        "            \n",
        "            if X_test and labels_test:\n",
        "              \n",
        "                y_hat_test = self.forward(X_test, keep_prob = 1)\n",
        "                \n",
        "                loss_test, delta_test=self.criterion_cross_entropy(y_test, y_hat_test)\n",
        "                \n",
        "                y_hat_label_test = y_hat_test.argmax(axis = 1)\n",
        "                \n",
        "                assert(len(y_hat_label_test) == len(labels_test))\n",
        "                \n",
        "                to_return_cost_test[k] = np.mean(loss_test)\n",
        "                \n",
        "                to_return_accuracy_test[k] = str(np.sum((y_hat_label_test == labels_test)/m_test))\n",
        "            \n",
        "            if k % 10000 ==0:\n",
        "                print(\"Cost after iteration %i: %f\" %(k, to_return_cost[k]))\n",
        "                print(\"Accuracy after iteration %i: %f\" %(k, to_return_accuracy[k]))\n",
        "\n",
        "        print(\"accuracy: \", to_return_accuracy[k])\n",
        "        print(\"cost: \", to_return_cost[k])\n",
        "        if X_test and labels_test: \n",
        "            print(\"accuracy_test: \", to_return_accuracy_test[k])\n",
        "            print(\"cost_test: \", to_return_cost_test[k])\n",
        "        \n",
        "        if X_test and labels_test:\n",
        "            return to_return_cost, to_return_accuracy, to_return_cost_test, to_return_accuracy_test, y_hat_label_test        \n",
        "        else:\n",
        "            return to_return_cost, to_return_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yw5Az6A97ONC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### This MLP_train altomatically trains the training data. just need to input number of inputs: m and other parameters."
      ]
    },
    {
      "metadata": {
        "id": "mWthkPtjeP9F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class MLP_train():\n",
        "    \"\"\"\n",
        "    hidden layers - a python list\n",
        "    \"\"\"\n",
        "    def __init__(self, m, num_features = 128, yn = 10):\n",
        "        \n",
        "        \n",
        "          \n",
        "        self.m = m\n",
        "        self.num_features = num_features\n",
        "        self.yn = yn\n",
        "\n",
        "        self.X_train = X[0:m,:]\n",
        "        self.labels_train = labels[0:m]\n",
        "   \n",
        "    def cal(self, mini_batch_size = 1, beta = .8, keep_prob = 1, learning_rate = .005, epochs = 100, hidden_layers = None):\n",
        "        \n",
        "        m = self.m\n",
        "        num_features = self.num_features\n",
        "        yn = self.yn\n",
        "        \n",
        "        if mini_batch_size > m: \n",
        "          raise ValueError('mini_batch_size too big')\n",
        "         \n",
        "        if hidden_layers == None:\n",
        "            layers_number = [num_features, yn]\n",
        "        else:\n",
        "            layers_number = [num_features] + hidden_layers + [yn]\n",
        "          \n",
        "        case = MLP(layers_number)\n",
        "        cost, accuracy = case.fit(self.X_train, self.labels_train, learning_rate, epochs, keep_prob, beta, mini_batch_size, X_test = None, labels_test= None)\n",
        "\n",
        "        %matplotlib inline\n",
        "        \n",
        "        pl.plot(cost)\n",
        "        pl.plot(accuracy)\n",
        "        \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q58XWD3d7ZLl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### input data from local"
      ]
    },
    {
      "metadata": {
        "id": "iJJW9uy3enXE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "cellView": "code",
        "outputId": "3dc16431-32ec-4528-af49-11427b786531",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525262798983,
          "user_tz": -600,
          "elapsed": 732082,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ad967a8-e68e-4e2d-b283-5048de71796e\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5ad967a8-e68e-4e2d-b283-5048de71796e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_128.h5 to train_128.h5\n",
            "Saving train_label.h5 to train_label.h5\n",
            "User uploaded file \"train_128.h5\" with length 61442144 bytes\n",
            "User uploaded file \"train_label.h5\" with length 482144 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a6KIqpFi7cxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### load in data"
      ]
    },
    {
      "metadata": {
        "id": "4TPsi98qepuS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "with h5py.File('train_128.h5','r') as H:\n",
        "    X = np.copy(H['data'])\n",
        "\n",
        "with h5py.File('train_label.h5','r') as H:\n",
        "    labels = np.copy(H['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6OmD8fjv7e8h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### normalise data: using z_score or normalisation method"
      ]
    },
    {
      "metadata": {
        "id": "9CdseuB22rIB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def z_score(x, axis):\n",
        "    x = np.array(x).astype(float)\n",
        "    xr = np.rollaxis(x, axis=axis)\n",
        "    xr -= np.mean(x, axis=axis)\n",
        "    xr /= np.std(x, axis=axis)\n",
        "    # print(x)\n",
        "    return x\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_JFkHvI5Emq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "needs to be debugged\n",
        "\"\"\"\n",
        "def norm(x, axis):\n",
        "    x = np.array(x).astype(float)\n",
        "    x_mean = np.mean(x, axis = axis)\n",
        "    x_max = np.max(x, axis = axis)\n",
        "    x_min = np.min(x, axis = axis)\n",
        "    x_ = (x-np.atleast_2d(x_mean))/(x_max-x_min)\n",
        "\n",
        "    # print(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KnLgSPHT5nft",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f548285b-fe92-4681-ff10-e32951f72344",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525266270376,
          "user_tz": -600,
          "elapsed": 732,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for testing \n",
        "\"\"\"\n",
        "x = np.array([[1,2,3],\n",
        "             [2,5,6]])\n",
        "z_score(x,1)\n",
        "norm(x,0)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 3.],\n",
              "       [2., 5., 6.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "metadata": {
        "id": "-sJrI_f53ZdQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "162b09bf-55cd-43ac-c334-2e7caf74163d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525265584064,
          "user_tz": -600,
          "elapsed": 994,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "for testing normalization methods on X\n",
        "\"\"\"\n",
        "X_norm = z_score(X, 1)\n",
        "pl.plot(X_norm[800])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa7a437a470>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAP9f9JC6ACwHGmxAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa7a4405588>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4XufGTyy7oHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### train the training data"
      ]
    },
    {
      "metadata": {
        "id": "5Bg6eg4BJV4J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "cellView": "code",
        "outputId": "b6173a70-2ede-4fc7-ecad-44ab178cebdb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525265600637,
          "user_tz": -600,
          "elapsed": 719,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "- the X_train is the data to be trained - can be itself, i.e. X_train, or X_norm, etc.\n",
        "- in test_1 = MLP_train(m= ). the m needs to be specified\n",
        "- in test_1.cal, parameters need to be specified\n",
        "- when encountered with runtime warning, it means either the normalization of X is not enough or learning rate is not appropriate\n",
        "\"\"\"\n",
        "X_train = X_norm\n",
        "test_1 = MLP_train(m=10)\n",
        "test_1.cal(mini_batch_size = 10, beta = .8, keep_prob = 1, learning_rate = .005, epochs = 2, hidden_layers = [5])"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: nan\n",
            "Accuracy after iteration 0: 0.000000\n",
            "accuracy:  0.0\n",
            "cost:  386.382842515974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in multiply\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD5CAYAAADY+KXfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nklQQQ12SCmKoS1JB/h8dnJoh3G+MjQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fa7a44055c0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Zwut_stWllMu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "test = MLP_train_random(m=100, num_features = 5, yn = 4)\n",
        "test.cal(mini_batch_size = 100, beta = 0.8, keep_prob = 1, learning_rate = 0.08, epochs = 500000, hidden_layers = [2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iq92ijJ5N4wI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "form"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "class MLP_train_random():\n",
        "    \"\"\"\n",
        "    hidden layers - a python list\n",
        "    \"\"\"\n",
        "    def __init__(self, m, num_features, yn):\n",
        "        \n",
        "        \n",
        "          \n",
        "        self.m = m\n",
        "        self.num_features = num_features\n",
        "        self.yn = yn\n",
        "        np.random.seed(1)\n",
        "        self.X_train = np.random.randint(num_features, size = (m, num_features))\n",
        "        self.labels_train = np.random.randint(yn, size = (m))\n",
        "    def cal(self, mini_batch_size = 1, beta = .8, keep_prob = 1, learning_rate = .005, epochs = 100, hidden_layers = None):\n",
        "      \n",
        "        m = self.m\n",
        "        num_features = self.num_features\n",
        "        yn = self.yn\n",
        "        \n",
        "        if mini_batch_size > m: \n",
        "          raise ValueError('mini_batch_size too big')\n",
        "         \n",
        "        if hidden_layers == None:\n",
        "            layers_number = [num_features, yn]\n",
        "        else:\n",
        "            layers_number = [num_features] + hidden_layers + [yn]\n",
        "          \n",
        "        case = MLP(layers_number)\n",
        "        cost, accuracy = case.fit(self.X_train, self.labels_train, learning_rate, epochs, keep_prob, beta, mini_batch_size, X_test = None, labels_test= None)\n",
        "\n",
        "        %matplotlib inline\n",
        "        \n",
        "        pl.plot(cost)\n",
        "        pl.plot(accuracy)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C00Q_2wSyT5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### full printing model: below code is just the above code with printing funcitons: intended for testing: it can overwrite the first three blocks"
      ]
    },
    {
      "metadata": {
        "id": "R0p06M4XyTBy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "import math\n",
        "\n",
        "class Activation(object):\n",
        "\n",
        "      \n",
        "    def __relu(self,x):\n",
        "        \n",
        "        return np.maximum(0,x)\n",
        "      \n",
        "    def __relu_deriv(self,a):\n",
        "        a[a>0]=1\n",
        "        a[a<=0]=0\n",
        "        return a\n",
        "      \n",
        "    def __softmax(self, x):\n",
        "        \"\"\"\n",
        "        x is of shape(m,n_in)\n",
        "        \"\"\"\n",
        "        y= np.exp(x)/np.sum(np.exp(x),axis=1).reshape(-1,1)\n",
        "        assert(y.shape == x.shape)\n",
        "        \n",
        "        return y\n",
        "      \n",
        "    def __softmax_deriv(self, a):\n",
        "        \n",
        "        y = a*(1-a)\n",
        "        assert(y.shape == a.shape)\n",
        "        return y\n",
        "\n",
        "    def __init__(self,activation='relu'):\n",
        "        if activation == 'softmax':\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv\n",
        "            \n",
        "class HiddenLayer(object):    \n",
        "    def __init__(self, n_in, n_out, W=None, b=None, activation = \"relu\" ):\n",
        "        \n",
        "        self.act = activation\n",
        "\n",
        "        self.input=None\n",
        "        self.activation = Activation(activation).f\n",
        "        self.activation_deriv = Activation(activation).f_deriv\n",
        "        # end-snippet-1\n",
        "\n",
        "        # `W` is initialized with `W_values` which is uniformely sampled\n",
        "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
        "        # Note : optimal initialization of weights is dependent on the\n",
        "        #        activation function used (among other things).\n",
        "        #        For example, results presented in [Xavier10] suggest that you\n",
        "        #        should use 4 times larger initial weights for sigmoid\n",
        "        #        compared to tanh\n",
        "        #        We have no info for other function, so we use the same as\n",
        "        #        tanh.\n",
        "        np.random.seed(7)\n",
        "        \n",
        "        self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out)),\n",
        "                high=np.sqrt(6. / (n_in + n_out)),\n",
        "                size=(n_in, n_out)\n",
        "        )\n",
        "\n",
        "       \n",
        "        self.b = np.zeros((1, n_out))\n",
        "        \n",
        "        print('1. initialized W: \\n', self.W)\n",
        "        print('2. initialzied b: \\n', self.b)\n",
        "        \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        self.v_dW = np.zeros(self.W.shape)\n",
        "        self.v_db = np.zeros(self.b.shape)\n",
        "        \n",
        "    def forward(self, input, keep_prob):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :input.shape = (m, n_in), b.shape = (1, n_out), W.shape = (n_in, n_out)\n",
        "        :lin_output.shape = (m, n_out)\n",
        "        '''\n",
        "\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        \n",
        "        print('forward 1. input: \\n', input)\n",
        "        print('forward 2. W: \\n', self.W)\n",
        "        print('forward 3. b: \\n', self.b)\n",
        "        print('forward 4, lin_output: \\n', lin_output)\n",
        "        \n",
        "        assert(self.W.shape[1] == self.b.shape[1])\n",
        "        assert(input.shape[1] == self.W.shape[0])\n",
        "        assert(input.shape[0] == lin_output.shape[0])\n",
        "        \n",
        "        if self.act == \"softmax\":\n",
        "            keep_prob = 1\n",
        "  \n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "\n",
        "        output = np.atleast_2d(self.output)\n",
        "\n",
        "        d = np.random.rand(output.shape[0],output.shape[1]) < keep_prob\n",
        "        output = np.multiply(d, output)\n",
        "        output /= keep_prob\n",
        "        print('forward 5, act_output: \\n', output)\n",
        "        assert(output.shape == self.output.shape)\n",
        "\n",
        "        self.input=input\n",
        "        self.output = output\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta, beta, keep_prob):       \n",
        "        \"\"\"\n",
        "        :self.input.shape = (m, n_in)\n",
        "        :delta.shape = (m, n_out)\n",
        "        :self.grad_W.shape = (n_in, n_out)\n",
        "        :self.grad_b.shape = (1, n_out)\n",
        "        :delta_.shape = (m, n_in)\n",
        "        \"\"\"\n",
        "        delta = np.atleast_2d(delta)\n",
        "        self.input = np.atleast_2d(self.input)\n",
        "        assert(self.input.shape[0] == delta.shape[0])\n",
        "        m = self.input.shape[0]\n",
        "        \n",
        "        print('backward 1. delta_in : \\n', delta)\n",
        "        print('backward 2. input : \\n', self.input)\n",
        "        \n",
        "        self.grad_W = self.input.T.dot(delta)/m\n",
        "        self.grad_b = np.sum(delta, axis = 0)/m    \n",
        "        self.grad_b = np.atleast_2d(self.grad_b)\n",
        "        \n",
        "\n",
        "        self.v_dW = beta * self.v_dW + (1-beta) * self.grad_W\n",
        "        self.v_db = beta * self.v_db + (1-beta) * self.grad_b\n",
        "\n",
        "        assert(self.v_dW.shape == self.grad_W.shape)\n",
        "        assert(self.v_db.shape == self.grad_b.shape)\n",
        "        \n",
        "        print('backward 3. self.grad_W  : \\n', self.grad_W )\n",
        "        print('backward 4. self.grad_b : \\n', self.grad_b)\n",
        "        print('backward 5. self.v_dW : \\n', self.v_dW)\n",
        "        print('backward 6. self.v_db : \\n', self.v_db)\n",
        "        \n",
        "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        delta_ /= keep_prob\n",
        "        assert(delta_.shape == self.input.shape)\n",
        "        \n",
        "        print('backward 7. self.W.T : \\n', self.W.T)\n",
        "        print('backward 8. self.activation_deriv(self.input) : \\n', self.activation_deriv(self.input))\n",
        "        print('backward 9. delta_ : \\n', delta_)\n",
        "        \n",
        "        return delta_\n",
        "\n",
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "\n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "\n",
        "        self.layers=[]\n",
        "        self.layers_number = layers\n",
        "        self.params=[]\n",
        "\n",
        "        self.activation= None \n",
        "        \n",
        "        for i in range(len(layers)-2):\n",
        "            print ('layer: ', i)\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1], activation=\"relu\"))\n",
        "\n",
        "        L = len(layers)\n",
        "        print ('layer: ', i)\n",
        "        self.layers.append(HiddenLayer(layers[L-2],layers[L-1], activation=\"softmax\"))\n",
        "            \n",
        "    def forward(self, input, keep_prob):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input, keep_prob)\n",
        "            input=output\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def criterion_cross_entropy(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        y.shape = y-hat.shape = (m, yn)\n",
        "        delta.shape = (m, yn)\n",
        "        \"\"\"\n",
        "      \n",
        "        assert(y.shape == y_hat.shape)\n",
        "        \n",
        "        activation_deriv=Activation(\"softmax\").f_deriv\n",
        "        \n",
        "        loss = -np.sum(np.log(y_hat) * y) \n",
        "                \n",
        "        error = y-y_hat\n",
        "        \n",
        "        delta = error*activation_deriv(y_hat)  \n",
        "        \n",
        "        print('criterion_cross_entropy 1. y: \\n', y)\n",
        "        print('criterion_cross_entropy 2. y_hat: \\n', y_hat)\n",
        "        print('criterion_cross_entropy 3. activation_deriv: \\n', activation_deriv)\n",
        "        print('criterion_cross_entropy 4. loss: \\n', loss)\n",
        "        print('criterion_cross_entropy 5. error: \\n', error)\n",
        "        print('criterion_cross_entropy 6. delta: \\n', delta)\n",
        "        \n",
        "        \n",
        "        assert(delta.shape == y.shape)\n",
        "        return loss, delta\n",
        "      \n",
        "    \n",
        "    def backward(self,delta, beta, keep_prob):        \n",
        "        for layer in reversed(self.layers):\n",
        "            delta=layer.backward(delta, beta, keep_prob)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        \"\"\"\n",
        "        v_db.shape = (m, n_out)\n",
        "        b.shape = (1, n_out)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            print('update 1. layer.W: \\n', layer.W)\n",
        "            print('update 2. layer.b: \\n', layer.b)\n",
        "            layer.W += lr * layer.v_dW                   \n",
        "            layer.b += lr * layer.v_db\n",
        "            \n",
        "            print('update 3. layer.v_dW : \\n', layer.v_dW)\n",
        "            print('update 4. layer.v_db: \\n', layer.v_db)\n",
        "            print('update 5. layer.W: \\n', layer.W)\n",
        "            print('update 6. layer.b: \\n', layer.b)\n",
        "            \n",
        "   \n",
        "    def create_mini_batch(self, mini_batch_size, labels, X, y, yn, num_features, m):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        #initializations\n",
        "        np.random.seed(1)\n",
        "\n",
        "        mini_batches = []            \n",
        "        permutation = list(np.random.permutation(m))\n",
        "        shuffled_X = X[permutation, :]\n",
        "        shuffled_y = y[permutation, :]\n",
        "        \n",
        "        labels = labels[permutation]\n",
        "        mini_batches_labels = labels\n",
        "\n",
        "        num_com_batches = math.floor(m/mini_batch_size)\n",
        "        \n",
        "        for k in range(0, num_com_batches):\n",
        "            mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)* mini_batch_size, : ]\n",
        "            mini_batch_y = shuffled_y[k*mini_batch_size:(k+1)* mini_batch_size, : ]\n",
        "            mini_batch = (mini_batch_X, mini_batch_y)\n",
        "            mini_batches.append(mini_batch)\n",
        "            assert(mini_batch_X.shape == (mini_batch_size, num_features))\n",
        "            assert(mini_batch_y.shape == (mini_batch_size, yn))\n",
        "            \n",
        "        if m % mini_batch_size !=0: \n",
        "            mini_batch_X = shuffled_X[(k+1)*mini_batch_size:, : ]\n",
        "            mini_batch_y = shuffled_y[(k+1)*mini_batch_size:, : ]\n",
        "            mini_batch = (mini_batch_X, mini_batch_y)\n",
        "            mini_batches.append(mini_batch)\n",
        "            \n",
        "        return mini_batches, mini_batches_labels\n",
        "      \n",
        "    def fit_pre_processing(self, X, labels, epochs):\n",
        "      \n",
        "        labels = labels.flatten()\n",
        "        assert(X.shape[0] == labels.shape[0])\n",
        "\n",
        "        y = (np.arange(self.layers_number[-1]) == labels[:, None]).astype(float)\n",
        "        yn = y.shape[1]\n",
        "        assert(y.shape[0] == X.shape[0])\n",
        "                \n",
        "        num_features = X.shape[1]       \n",
        "        m = X.shape[0]\n",
        "        \n",
        "        to_return_cost = np.zeros(epochs)\n",
        "        to_return_accuracy = np.zeros(epochs)\n",
        "        \n",
        "        return y, yn, num_features, m, to_return_cost, to_return_accuracy\n",
        "      \n",
        "    def fit(self, X, labels, learning_rate=0.1, epochs=100, keep_prob = 1, beta=0, mini_batch_size = 1, X_test = None, labels_test = None, ):\n",
        "        \n",
        "        \"\"\"\n",
        "        :X.shape = (m, num_features)\n",
        "        :labels.shape = (m, 1) / (m,)\n",
        "        :y.shape = (m, yn)\n",
        "        :y_hat.shape = y.shape\n",
        "        \"\"\"\n",
        "        y, yn, num_features, m, to_return_cost, to_return_accuracy = self.fit_pre_processing(X, labels, epochs)\n",
        "        \n",
        "        if X_test and labels_test:\n",
        "            y_test, yn_test, num_features_test, m_test, to_return_cost_test, to_return_accuracy_test = self.fit_pre_processing(X_test, labels_test, epochs)\n",
        "\n",
        "            assert(num_features == num_features_test)\n",
        "            assert(yn == yn_test)\n",
        "\n",
        "            num_features_test = num_features\n",
        "            yn_test = yn\n",
        "\n",
        "        mini_batches, mini_batches_labels = self.create_mini_batch(mini_batch_size, labels, X, y, yn, num_features, m)\n",
        "\n",
        "        for k in range(epochs):\n",
        "          \n",
        "            print('epoch ', k, ':')\n",
        "            y_hat_labels = []\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                \n",
        "                mini_batch_X, mini_batch_y = mini_batch\n",
        "                \n",
        "                mini_batch_y_hat = self.forward(mini_batch_X, keep_prob)\n",
        "                \n",
        "                loss, delta=self.criterion_cross_entropy(mini_batch_y, mini_batch_y_hat)\n",
        "                \n",
        "                self.backward(delta, beta, keep_prob)\n",
        "                \n",
        "                self.update(learning_rate)\n",
        "                \n",
        "                y_hat_label = mini_batch_y_hat.argmax(axis = 1)\n",
        "                \n",
        "                y_hat_labels = np.append(y_hat_labels, y_hat_label)\n",
        "    \n",
        "            to_return_cost[k] = np.mean(loss)\n",
        "            \n",
        "            assert(len(y_hat_labels) == len(mini_batches_labels))\n",
        "            to_return_accuracy[k] = str(np.sum((y_hat_labels == mini_batches_labels)/m)) \n",
        "            \n",
        "            if X_test and labels_test:\n",
        "              \n",
        "                y_hat_test = self.forward(X_test, keep_prob = 1)\n",
        "                \n",
        "                loss_test, delta_test=self.criterion_cross_entropy(y_test, y_hat_test)\n",
        "                \n",
        "                y_hat_label_test = y_hat_test.argmax(axis = 1)\n",
        "                \n",
        "                assert(len(y_hat_label_test) == len(labels_test))\n",
        "                \n",
        "                to_return_cost_test[k] = np.mean(loss_test)\n",
        "                \n",
        "                to_return_accuracy_test[k] = str(np.sum((y_hat_label_test == labels_test)/m_test))\n",
        "            \n",
        "            if k % 100 ==0:\n",
        "                print(\"Cost after iteration %i: %f\" %(k, to_return_cost[k]))\n",
        "                print(\"Accuracy after iteration %i: %f\" %(k, to_return_accuracy[k]))\n",
        "\n",
        "        print(\"accuracy: \", to_return_accuracy[k])\n",
        "        print(\"cost: \", to_return_cost[k])\n",
        "        if X_test and labels_test: \n",
        "            print(\"accuracy_test: \", to_return_accuracy_test[k])\n",
        "            print(\"cost_test: \", to_return_cost_test[k])\n",
        "        \n",
        "        if X_test and labels_test:\n",
        "            return to_return_cost, to_return_accuracy, to_return_cost_test, to_return_accuracy_test, y_hat_label_test        \n",
        "        else:\n",
        "            return to_return_cost, to_return_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}