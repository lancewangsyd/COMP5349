{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ass1_new_version1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8lh3M9wZ4rhf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Activation(object):\n",
        "\n",
        "      \n",
        "    def __relu(self,x):\n",
        "        \n",
        "        return np.maximum(0,x)\n",
        "      \n",
        "    def __relu_deriv(self,a):\n",
        "        a[a>=0]=1\n",
        "        a[a<0]=0\n",
        "        return a\n",
        "      \n",
        "    def __softmax(self, x):\n",
        "        \"\"\"\n",
        "        x is of shape(m,n_in)\n",
        "        \"\"\"\n",
        "        y= np.exp(x)/np.sum(np.exp(x),axis=1).reshape(-1,1)\n",
        "        assert(y.shape == x.shape)\n",
        "        \n",
        "        return y\n",
        "      \n",
        "    def __softmax_deriv(self, a):\n",
        "        \n",
        "        y = a*(1-a)\n",
        "        assert(y.shape == a.shape)\n",
        "        return y\n",
        "\n",
        "    def __init__(self,activation='relu'):\n",
        "        if activation == 'softmax':\n",
        "            self.f = self.__softmax\n",
        "            self.f_deriv = self.__softmax_deriv\n",
        "        elif activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "53mdNGVZWxEr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 10
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "d9694a3a-3eeb-477e-abed-9e9a06b1cc34",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525155828353,
          "user_tz": -600,
          "elapsed": 693,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "np.exp(q)/np.sum(np.exp(q),axis=1).reshape(-1,1)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.01606255, 0.32262497, 0.32262497, 0.01606255, 0.32262497],\n",
              "       [0.10501196, 0.03863174, 0.2854521 , 0.2854521 , 0.2854521 ],\n",
              "       [0.01499127, 0.11077134, 0.0407505 , 0.81849562, 0.01499127]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "metadata": {
        "id": "skcEQOleWrhj",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class HiddenLayer(object):    \n",
        "    def __init__(self, n_in, n_out, keep_prob, beta = 0, W=None, b=None, activation = \"relu\" ):\n",
        "        \"\"\"\n",
        "        Typical hidden layer of a MLP: units are fully-connected and have\n",
        "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
        "        and the bias vector b is of shape (n_out,).\n",
        "\n",
        "        NOTE : The nonlinearity used here is tanh\n",
        "\n",
        "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
        "\n",
        "        :type n_in: int\n",
        "        :param n_in: dimensionality of input\n",
        "\n",
        "        :type n_out: int\n",
        "        :param n_out: number of hidden units\n",
        "\n",
        "        :type activation: string\n",
        "        :param activation: Non linearity to be applied in the hidden\n",
        "                           layer\n",
        "        \"\"\"\n",
        "        \n",
        "        self.act = activation\n",
        "        self.keep_prob  = keep_prob\n",
        "        self.beta = beta\n",
        "        self.input=None\n",
        "        self.activation = Activation(activation).f\n",
        "        self.activation_deriv = Activation(activation).f_deriv\n",
        "        # end-snippet-1\n",
        "\n",
        "        # `W` is initialized with `W_values` which is uniformely sampled\n",
        "        # from sqrt(-6./(n_in+n_hidden)) and sqrt(6./(n_in+n_hidden))\n",
        "        # Note : optimal initialization of weights is dependent on the\n",
        "        #        activation function used (among other things).\n",
        "        #        For example, results presented in [Xavier10] suggest that you\n",
        "        #        should use 4 times larger initial weights for sigmoid\n",
        "        #        compared to tanh\n",
        "        #        We have no info for other function, so we use the same as\n",
        "        #        tanh.\n",
        "        np.random.seed(1)\n",
        "        self.W = np.random.uniform(\n",
        "                low=-np.sqrt(6. / (n_in + n_out))*0.01,\n",
        "                high=np.sqrt(6. / (n_in + n_out))*0.01,\n",
        "                size=(n_in, n_out)\n",
        "        )\n",
        "        \n",
        "\n",
        "\n",
        "        self.b = np.zeros((1, n_out))\n",
        "        \n",
        "        self.grad_W = np.zeros(self.W.shape)\n",
        "        self.grad_b = np.zeros(self.b.shape)\n",
        "        \n",
        "        self.v_dW = np.zeros(self.W.shape)\n",
        "        self.v_db = np.zeros(self.b.shape)\n",
        "        \n",
        "        #print(\"v_db.shape: \", self.v_db.shape)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        '''\n",
        "        :type input: numpy.array\n",
        "        :input.shape = (m, n_in), b.shape = (1, n_out), W.shape = (n_in, n_out)\n",
        "        :lin_output.shape = (m, n_out)\n",
        "        '''\n",
        "\n",
        "        lin_output = np.dot(input, self.W) + self.b\n",
        "        \n",
        "        assert(self.W.shape[1] == self.b.shape[1])\n",
        "        assert(input.shape[1] == self.W.shape[0])\n",
        "        assert(input.shape[0] == lin_output.shape[0])\n",
        "  \n",
        "        self.output = (\n",
        "            lin_output if self.activation is None\n",
        "            else self.activation(lin_output)\n",
        "        )\n",
        "\n",
        "        keep_prob = self.keep_prob\n",
        "        output = np.atleast_2d(self.output)\n",
        "\n",
        "        d = np.random.rand(output.shape[0],output.shape[1]) < keep_prob\n",
        "        output = np.multiply(d, output)\n",
        "        output /= keep_prob\n",
        "        \n",
        "        assert(output.shape == self.output.shape)\n",
        "\n",
        "        self.input=input\n",
        "        self.output = output\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, delta):       \n",
        "        \"\"\"\n",
        "        :self.input.shape = (m, n_in)\n",
        "        :delta.shape = (m, n_out)\n",
        "        :self.grad_W.shape = (n_in, n_out)\n",
        "        :self.grad_b.shape = (1, n_out)\n",
        "        :delta_.shape = (m, n_in)\n",
        "        \"\"\"\n",
        "        delta = np.atleast_2d(delta)\n",
        "        self.input = np.atleast_2d(self.input)\n",
        "        assert(self.input.shape[0] == delta.shape[0])\n",
        "        m = self.input.shape[0]\n",
        "        \n",
        "        self.grad_W = self.input.T.dot(delta)/m\n",
        "        self.grad_b = np.sum(delta, axis = 0)/m\n",
        "        \n",
        "        self.grad_b = np.atleast_2d(self.grad_b)\n",
        "        \n",
        "        \n",
        "        beta = self.beta\n",
        "\n",
        "        self.v_dW = beta * self.v_dW + (1-beta) * self.grad_W\n",
        "        self.v_db = beta * self.v_db + (1-beta) * self.grad_b\n",
        "\n",
        "        assert(self.v_dW.shape == self.grad_W.shape)\n",
        "        assert(self.v_db.shape == self.grad_b.shape)\n",
        "        \n",
        "        keep_prob = self.keep_prob\n",
        "        \n",
        "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
        "        delta_ /= keep_prob\n",
        "        assert(delta_.shape == self.input.shape)\n",
        "        return delta_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MG0wz03vyvbQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 10
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "7c1f7bc7-b2c5-4ff0-e913-1c4a29457c4f",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525150519144,
          "user_tz": -600,
          "elapsed": 949,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import math\n",
        "np.random.seed(1)\n",
        "m=5\n",
        "X = np.random.rand(5,2)\n",
        "labels = np.random.randint(5, size = (5,1))\n",
        "y=  (np.arange(5) == labels).astype(float)\n",
        "permutation = list(np.random.permutation(m))\n",
        "shuffled_X = X[permutation, :]\n",
        "shuffled_y = y[permutation, :]\n",
        "\n",
        "minibatch_size = 2\n",
        "num_com_batches = math.floor(m/minibatch_size)\n",
        "minibatches = []      \n",
        "for k in range(0, num_com_batches):\n",
        "    mini_batch_X = shuffled_X[k*minibatch_size:(k+1)* minibatch_size, : ]\n",
        "    mini_batch_y = shuffled_y[k*minibatch_size:(k+1)* minibatch_size, : ]\n",
        "    mini_batch = (mini_batch_X, mini_batch_y)\n",
        "    minibatches.append(mini_batch)\n",
        "\n",
        "if m % minibatch_size !=0: \n",
        "    mini_batch_X = shuffled_X[(k+1)*minibatch_size:, : ]\n",
        "    mini_batch_y = shuffled_y[(k+1)*minibatch_size:, : ]\n",
        "    mini_batch = (mini_batch_X, mini_batch_y)\n",
        "    minibatches.append(mini_batch)\n",
        "for i in minibatches:\n",
        "  \n",
        "  print(i)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([[0.18626021, 0.34556073],\n",
            "       [0.417022  , 0.72032449]]), array([[0., 0., 0., 0., 1.],\n",
            "       [0., 0., 1., 0., 0.]]))\n",
            "(array([[1.46755891e-01, 9.23385948e-02],\n",
            "       [1.14374817e-04, 3.02332573e-01]]), array([[0., 0., 0., 1., 0.],\n",
            "       [0., 0., 0., 0., 1.]]))\n",
            "(array([[0.39676747, 0.53881673]]), array([[0., 0., 1., 0., 0.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GjkBweoHBh02",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7d67caf1-27d3-4a6f-a250-e0711648d9e4",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525177377264,
          "user_tz": -600,
          "elapsed": 1310,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    \"\"\"      \n",
        "    def __init__(self, layers, keep_prob, beta=0, mini_batch_size = 1):\n",
        "        \"\"\"\n",
        "        :param layers: A list containing the number of units in each layer.\n",
        "        Should be at least two values\n",
        "\n",
        "        \"\"\"        \n",
        "        ### initialize layers\n",
        "        self.labels = None\n",
        "\n",
        "        self.X =None\n",
        "        self.mini_batch_size = mini_batch_size\n",
        "        self.y = None\n",
        "        self.layers=[]\n",
        "        self.layers_number = layers\n",
        "        self.params=[]\n",
        "        self.beta = beta\n",
        "        self.keep_prob = keep_prob\n",
        "        \n",
        "        \n",
        "        self.num_features = None\n",
        "        self.yn=None\n",
        "        self.m=None\n",
        "        self.activation= None \n",
        "        \n",
        "        for i in range(len(layers)-2):\n",
        "            self.layers.append(HiddenLayer(layers[i],layers[i+1], activation=\"relu\", beta = self.beta, keep_prob = self.keep_prob))\n",
        "\n",
        "        L = len(layers)\n",
        "        self.layers.append(HiddenLayer(layers[L-2],layers[L-1], activation=\"softmax\", beta = self.beta, keep_prob =1))\n",
        "            \n",
        "    def forward(self,input):\n",
        "        for layer in self.layers:\n",
        "            output=layer.forward(input)\n",
        "            input=output\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def criterion_cross_entropy(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        y.shape = y-hat.shape = (m, yn)\n",
        "        delta.shape = (m, yn)\n",
        "        \"\"\"\n",
        "      \n",
        "        assert(y.shape == y_hat.shape)\n",
        "        \n",
        "        activation_deriv=Activation(\"softmax\").f_deriv\n",
        "        \n",
        "        loss = -np.sum(np.log(y_hat) * y) \n",
        "                \n",
        "        error = y-y_hat\n",
        "        \n",
        "        delta = error*activation_deriv(y_hat)  \n",
        "        assert(delta.shape == y.shape)\n",
        "        return loss, delta\n",
        "      \n",
        "    \n",
        "    def backward(self,delta):        \n",
        "        for layer in reversed(self.layers):\n",
        "            delta=layer.backward(delta)\n",
        "            \n",
        "    def update(self,lr):\n",
        "        \"\"\"\n",
        "        v_db.shape = (m, n_out)\n",
        "        b.shape = (1, n_out)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            layer.W += lr * layer.v_dW                   \n",
        "            layer.b += lr * layer.v_db\n",
        "            \n",
        "   \n",
        "    def create_mini_batch(self):\n",
        "        \"\"\"\n",
        "        \n",
        "        \"\"\"\n",
        "        #initializations\n",
        "        np.random.seed(1)\n",
        "        m = self.m\n",
        "        mini_batch_size = self.mini_batch_size\n",
        "        labels = self.labels\n",
        "        mini_batches = []        \n",
        "        y = self.y        \n",
        "        X = self.X\n",
        "        yn = self.yn\n",
        "        num_features = self.num_features\n",
        "        \n",
        "        permutation = list(np.random.permutation(m))\n",
        "        shuffled_X = X[permutation, :]\n",
        "        shuffled_y = y[permutation, :]\n",
        "        \n",
        "        labels = labels[permutation]\n",
        "        mini_batches_labels = labels\n",
        "\n",
        "        num_com_batches = math.floor(m/mini_batch_size)\n",
        "        \n",
        "        for k in range(0, num_com_batches):\n",
        "            mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)* mini_batch_size, : ]\n",
        "            mini_batch_y = shuffled_y[k*mini_batch_size:(k+1)* mini_batch_size, : ]\n",
        "            mini_batch = (mini_batch_X, mini_batch_y)\n",
        "            mini_batches.append(mini_batch)\n",
        "            assert(mini_batch_X.shape == (mini_batch_size, num_features))\n",
        "            assert(mini_batch_y.shape == (mini_batch_size, yn))\n",
        "            \n",
        "        if m % mini_batch_size !=0: \n",
        "            mini_batch_X = shuffled_X[(k+1)*mini_batch_size:, : ]\n",
        "            mini_batch_y = shuffled_y[(k+1)*mini_batch_size:, : ]\n",
        "            mini_batch = (mini_batch_X, mini_batch_y)\n",
        "            mini_batches.append(mini_batch)\n",
        "            \n",
        "        return mini_batches, mini_batches_labels\n",
        "      \n",
        "    \n",
        "    \n",
        "    def fit(self, X, labels, learning_rate=0.1, epochs=100):\n",
        "        \n",
        "        \"\"\"\n",
        "        :X.shape = (m, num_features)\n",
        "        :labels.shape = (m, 1) / (m,)\n",
        "        :y.shape = (m, yn)\n",
        "        :y_hat.shape = y.shape\n",
        "        \"\"\"\n",
        "        self.X = X\n",
        "        labels = labels.flatten()\n",
        "        self.labels = labels\n",
        "        assert(X.shape[0] == labels.shape[0])\n",
        "        \n",
        "        \n",
        "        y = (np.arange(self.layers_number[-1]) == labels[:, None]).astype(float)\n",
        "        self.y = y\n",
        "        self.yn = y.shape[1]\n",
        "        assert(y.shape[0] == X.shape[0])\n",
        "                \n",
        "        num_features = X.shape[1]\n",
        "        self.num_features = num_features\n",
        "       \n",
        "        m = X.shape[0]\n",
        "        self.m = m\n",
        "        \n",
        "        to_return_cost = np.zeros(epochs)\n",
        "        to_return_accuracy = np.zeros(epochs)\n",
        "        \n",
        "       \n",
        "        mini_batches, mini_batches_labels = self.create_mini_batch()\n",
        "\n",
        "        for k in range(epochs):\n",
        "            p_labels = []\n",
        "\n",
        "            for mini_batch in mini_batches:\n",
        "                \n",
        "                mini_batch_X, mini_batch_y = mini_batch\n",
        "                \n",
        "                mini_batch_y_hat = self.forward(mini_batch_X)\n",
        "                \n",
        "                loss, delta=self.criterion_cross_entropy(mini_batch_y, mini_batch_y_hat)\n",
        "                \n",
        "                self.backward(delta)\n",
        "                \n",
        "                self.update(learning_rate)\n",
        "                \n",
        "                p = mini_batch_y_hat.argmax(axis = 1)\n",
        "                \n",
        "                p_labels = np.append(p_labels, p)\n",
        "    \n",
        "            to_return_cost[k] = np.mean(loss)\n",
        "            \n",
        "            assert(len(p_labels) == len(mini_batches_labels))\n",
        "            to_return_accuracy[k] = str(np.sum((p_labels == mini_batches_labels)/m)) \n",
        "            \n",
        "            if k % 100 ==0:\n",
        "              print(\"Cost after iteration %i: %f\" %(k, to_return_cost[k]))\n",
        "              print(\"Accuracy after iteration %i: %f\" %(k, to_return_accuracy[k]))\n",
        "\n",
        "        print(\"accuracy: \", to_return_accuracy[k])\n",
        "        print(\"cost: \", to_return_cost[k])\n",
        "        \n",
        "        return to_return_cost, to_return_accuracy\n",
        "      \n",
        "      \n",
        "\n",
        "\"\"\" def predict(self, X_test, y_test):\n",
        "\n",
        "    X_test = np.array(X_test)\n",
        "    yn=self.yn\n",
        "    m=self.m\n",
        "    output = np.zeros([m,yn])\n",
        "\n",
        "    self.forward().keep_prob = 1\n",
        "    for i in np.arange(m):\n",
        "        output[i] = self.forward(x[i,:])\n",
        "\n",
        "        return output\"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' def predict(self, X_test, y_test):\\n\\n    X_test = np.array(X_test)\\n    yn=self.yn\\n    m=self.m\\n    output = np.zeros([m,yn])\\n\\n    self.forward().keep_prob = 1\\n    for i in np.arange(m):\\n        output[i] = self.forward(x[i,:])\\n\\n        return output'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "mWthkPtjeP9F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class MLP_train():\n",
        "    \"\"\"\n",
        "    hidden layers - a python list\n",
        "    \"\"\"\n",
        "    def __init__(self, m, num_features = 128, yn = 10):\n",
        "        \n",
        "        \n",
        "          \n",
        "        self.m = m\n",
        "        self.num_features = num_features\n",
        "        self.yn = yn\n",
        "\n",
        "        self.X_train = X[0:m,:]\n",
        "        self.labels_train = labels[0:m]\n",
        "   \n",
        "    def fit(self, mini_batch_size = 1, beta = .8, keep_prob = 1, learning_rate = .005, epochs = 100, hidden_layers = None):\n",
        "        \n",
        "        m = self.m\n",
        "        num_features = self.num_features\n",
        "        yn = self.yn\n",
        "        \n",
        "        if mini_batch_size > m: \n",
        "          raise ValueError('mini_batch_size too big')\n",
        "         \n",
        "        if hidden_layers == None:\n",
        "            layers_number = [num_features, yn]\n",
        "        else:\n",
        "            layers_number = [num_features] + hidden_layers + [yn]\n",
        "          \n",
        "        case = MLP(layers_number, keep_prob, beta, mini_batch_size)\n",
        "        cost, accuracy = case.fit(self.X_train, self.labels_train, learning_rate, epochs)\n",
        "\n",
        "        %matplotlib inline\n",
        "        \n",
        "        pl.plot(cost)\n",
        "        pl.plot(accuracy)\n",
        "        \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iJJW9uy3enXE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "741517de-45cb-4a23-bfeb-9947fe88aad0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525178275663,
          "user_tz": -600,
          "elapsed": 730189,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3d45afdb-d623-4680-983e-55db4c857800\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3d45afdb-d623-4680-983e-55db4c857800\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train_128.h5 to train_128.h5\n",
            "Saving train_label.h5 to train_label.h5\n",
            "User uploaded file \"train_128.h5\" with length 61442144 bytes\n",
            "User uploaded file \"train_label.h5\" with length 482144 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4TPsi98qepuS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "with h5py.File('train_128.h5','r') as H:\n",
        "    X = np.copy(H['data'])\n",
        "\n",
        "with h5py.File('train_label.h5','r') as H:\n",
        "    labels = np.copy(H['label'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Bg6eg4BJV4J",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3699
        },
        "outputId": "75dec029-f7ca-4c11-e644-4dd31c5abf7e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525179649797,
          "user_tz": -600,
          "elapsed": 6029,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as pl\n",
        "import math\n",
        "test_1 = MLP_train(m=32)\n",
        "test_1.fit(epochs = 10000, mini_batch_size = 32, hidden_layers = [5,2,5], keep_prob = 0.5, learning_rate = .00005, beta = 0.9)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 73.682723\n",
            "Accuracy after iteration 0: 0.187500\n",
            "Cost after iteration 100: 73.682370\n",
            "Accuracy after iteration 100: 0.187500\n",
            "Cost after iteration 200: 73.681982\n",
            "Accuracy after iteration 200: 0.187500\n",
            "Cost after iteration 300: 73.681594\n",
            "Accuracy after iteration 300: 0.187500\n",
            "Cost after iteration 400: 73.681206\n",
            "Accuracy after iteration 400: 0.187500\n",
            "Cost after iteration 500: 73.680818\n",
            "Accuracy after iteration 500: 0.187500\n",
            "Cost after iteration 600: 73.680430\n",
            "Accuracy after iteration 600: 0.187500\n",
            "Cost after iteration 700: 73.680042\n",
            "Accuracy after iteration 700: 0.187500\n",
            "Cost after iteration 800: 73.679654\n",
            "Accuracy after iteration 800: 0.187500\n",
            "Cost after iteration 900: 73.679266\n",
            "Accuracy after iteration 900: 0.187500\n",
            "Cost after iteration 1000: 73.678878\n",
            "Accuracy after iteration 1000: 0.187500\n",
            "Cost after iteration 1100: 73.678490\n",
            "Accuracy after iteration 1100: 0.187500\n",
            "Cost after iteration 1200: 73.678103\n",
            "Accuracy after iteration 1200: 0.187500\n",
            "Cost after iteration 1300: 73.677715\n",
            "Accuracy after iteration 1300: 0.187500\n",
            "Cost after iteration 1400: 73.677327\n",
            "Accuracy after iteration 1400: 0.187500\n",
            "Cost after iteration 1500: 73.676940\n",
            "Accuracy after iteration 1500: 0.187500\n",
            "Cost after iteration 1600: 73.676552\n",
            "Accuracy after iteration 1600: 0.187500\n",
            "Cost after iteration 1700: 73.676165\n",
            "Accuracy after iteration 1700: 0.187500\n",
            "Cost after iteration 1800: 73.675777\n",
            "Accuracy after iteration 1800: 0.187500\n",
            "Cost after iteration 1900: 73.675389\n",
            "Accuracy after iteration 1900: 0.187500\n",
            "Cost after iteration 2000: 73.675002\n",
            "Accuracy after iteration 2000: 0.187500\n",
            "Cost after iteration 2100: 73.674615\n",
            "Accuracy after iteration 2100: 0.187500\n",
            "Cost after iteration 2200: 73.674227\n",
            "Accuracy after iteration 2200: 0.187500\n",
            "Cost after iteration 2300: 73.673840\n",
            "Accuracy after iteration 2300: 0.187500\n",
            "Cost after iteration 2400: 73.673452\n",
            "Accuracy after iteration 2400: 0.187500\n",
            "Cost after iteration 2500: 73.673065\n",
            "Accuracy after iteration 2500: 0.187500\n",
            "Cost after iteration 2600: 73.672678\n",
            "Accuracy after iteration 2600: 0.187500\n",
            "Cost after iteration 2700: 73.672291\n",
            "Accuracy after iteration 2700: 0.187500\n",
            "Cost after iteration 2800: 73.671903\n",
            "Accuracy after iteration 2800: 0.187500\n",
            "Cost after iteration 2900: 73.671516\n",
            "Accuracy after iteration 2900: 0.187500\n",
            "Cost after iteration 3000: 73.671129\n",
            "Accuracy after iteration 3000: 0.187500\n",
            "Cost after iteration 3100: 73.670742\n",
            "Accuracy after iteration 3100: 0.187500\n",
            "Cost after iteration 3200: 73.670355\n",
            "Accuracy after iteration 3200: 0.187500\n",
            "Cost after iteration 3300: 73.669968\n",
            "Accuracy after iteration 3300: 0.187500\n",
            "Cost after iteration 3400: 73.669581\n",
            "Accuracy after iteration 3400: 0.187500\n",
            "Cost after iteration 3500: 73.669194\n",
            "Accuracy after iteration 3500: 0.187500\n",
            "Cost after iteration 3600: 73.668807\n",
            "Accuracy after iteration 3600: 0.187500\n",
            "Cost after iteration 3700: 73.668420\n",
            "Accuracy after iteration 3700: 0.187500\n",
            "Cost after iteration 3800: 73.668033\n",
            "Accuracy after iteration 3800: 0.187500\n",
            "Cost after iteration 3900: 73.667646\n",
            "Accuracy after iteration 3900: 0.187500\n",
            "Cost after iteration 4000: 73.667259\n",
            "Accuracy after iteration 4000: 0.187500\n",
            "Cost after iteration 4100: 73.666873\n",
            "Accuracy after iteration 4100: 0.187500\n",
            "Cost after iteration 4200: 73.666486\n",
            "Accuracy after iteration 4200: 0.187500\n",
            "Cost after iteration 4300: 73.666099\n",
            "Accuracy after iteration 4300: 0.187500\n",
            "Cost after iteration 4400: 73.665712\n",
            "Accuracy after iteration 4400: 0.187500\n",
            "Cost after iteration 4500: 73.665326\n",
            "Accuracy after iteration 4500: 0.187500\n",
            "Cost after iteration 4600: 73.664939\n",
            "Accuracy after iteration 4600: 0.187500\n",
            "Cost after iteration 4700: 73.664553\n",
            "Accuracy after iteration 4700: 0.187500\n",
            "Cost after iteration 4800: 73.664166\n",
            "Accuracy after iteration 4800: 0.187500\n",
            "Cost after iteration 4900: 73.663779\n",
            "Accuracy after iteration 4900: 0.187500\n",
            "Cost after iteration 5000: 73.663393\n",
            "Accuracy after iteration 5000: 0.187500\n",
            "Cost after iteration 5100: 73.663007\n",
            "Accuracy after iteration 5100: 0.187500\n",
            "Cost after iteration 5200: 73.662620\n",
            "Accuracy after iteration 5200: 0.187500\n",
            "Cost after iteration 5300: 73.662234\n",
            "Accuracy after iteration 5300: 0.187500\n",
            "Cost after iteration 5400: 73.661847\n",
            "Accuracy after iteration 5400: 0.187500\n",
            "Cost after iteration 5500: 73.661461\n",
            "Accuracy after iteration 5500: 0.187500\n",
            "Cost after iteration 5600: 73.661075\n",
            "Accuracy after iteration 5600: 0.187500\n",
            "Cost after iteration 5700: 73.660689\n",
            "Accuracy after iteration 5700: 0.187500\n",
            "Cost after iteration 5800: 73.660302\n",
            "Accuracy after iteration 5800: 0.187500\n",
            "Cost after iteration 5900: 73.659916\n",
            "Accuracy after iteration 5900: 0.187500\n",
            "Cost after iteration 6000: 73.659530\n",
            "Accuracy after iteration 6000: 0.187500\n",
            "Cost after iteration 6100: 73.659144\n",
            "Accuracy after iteration 6100: 0.187500\n",
            "Cost after iteration 6200: 73.658758\n",
            "Accuracy after iteration 6200: 0.187500\n",
            "Cost after iteration 6300: 73.658372\n",
            "Accuracy after iteration 6300: 0.187500\n",
            "Cost after iteration 6400: 73.657986\n",
            "Accuracy after iteration 6400: 0.187500\n",
            "Cost after iteration 6500: 73.657600\n",
            "Accuracy after iteration 6500: 0.187500\n",
            "Cost after iteration 6600: 73.657214\n",
            "Accuracy after iteration 6600: 0.187500\n",
            "Cost after iteration 6700: 73.656828\n",
            "Accuracy after iteration 6700: 0.187500\n",
            "Cost after iteration 6800: 73.656442\n",
            "Accuracy after iteration 6800: 0.187500\n",
            "Cost after iteration 6900: 73.656056\n",
            "Accuracy after iteration 6900: 0.187500\n",
            "Cost after iteration 7000: 73.655670\n",
            "Accuracy after iteration 7000: 0.187500\n",
            "Cost after iteration 7100: 73.655284\n",
            "Accuracy after iteration 7100: 0.187500\n",
            "Cost after iteration 7200: 73.654899\n",
            "Accuracy after iteration 7200: 0.187500\n",
            "Cost after iteration 7300: 73.654513\n",
            "Accuracy after iteration 7300: 0.187500\n",
            "Cost after iteration 7400: 73.654127\n",
            "Accuracy after iteration 7400: 0.187500\n",
            "Cost after iteration 7500: 73.653742\n",
            "Accuracy after iteration 7500: 0.187500\n",
            "Cost after iteration 7600: 73.653356\n",
            "Accuracy after iteration 7600: 0.187500\n",
            "Cost after iteration 7700: 73.652970\n",
            "Accuracy after iteration 7700: 0.187500\n",
            "Cost after iteration 7800: 73.652585\n",
            "Accuracy after iteration 7800: 0.187500\n",
            "Cost after iteration 7900: 73.652199\n",
            "Accuracy after iteration 7900: 0.187500\n",
            "Cost after iteration 8000: 73.651814\n",
            "Accuracy after iteration 8000: 0.187500\n",
            "Cost after iteration 8100: 73.651428\n",
            "Accuracy after iteration 8100: 0.187500\n",
            "Cost after iteration 8200: 73.651043\n",
            "Accuracy after iteration 8200: 0.187500\n",
            "Cost after iteration 8300: 73.650658\n",
            "Accuracy after iteration 8300: 0.187500\n",
            "Cost after iteration 8400: 73.650272\n",
            "Accuracy after iteration 8400: 0.187500\n",
            "Cost after iteration 8500: 73.649887\n",
            "Accuracy after iteration 8500: 0.187500\n",
            "Cost after iteration 8600: 73.649502\n",
            "Accuracy after iteration 8600: 0.187500\n",
            "Cost after iteration 8700: 73.649116\n",
            "Accuracy after iteration 8700: 0.187500\n",
            "Cost after iteration 8800: 73.648731\n",
            "Accuracy after iteration 8800: 0.187500\n",
            "Cost after iteration 8900: 73.648346\n",
            "Accuracy after iteration 8900: 0.187500\n",
            "Cost after iteration 9000: 73.647961\n",
            "Accuracy after iteration 9000: 0.187500\n",
            "Cost after iteration 9100: 73.647576\n",
            "Accuracy after iteration 9100: 0.187500\n",
            "Cost after iteration 9200: 73.647191\n",
            "Accuracy after iteration 9200: 0.187500\n",
            "Cost after iteration 9300: 73.646805\n",
            "Accuracy after iteration 9300: 0.187500\n",
            "Cost after iteration 9400: 73.646420\n",
            "Accuracy after iteration 9400: 0.187500\n",
            "Cost after iteration 9500: 73.646035\n",
            "Accuracy after iteration 9500: 0.187500\n",
            "Cost after iteration 9600: 73.645650\n",
            "Accuracy after iteration 9600: 0.187500\n",
            "Cost after iteration 9700: 73.645266\n",
            "Accuracy after iteration 9700: 0.187500\n",
            "Cost after iteration 9800: 73.644881\n",
            "Accuracy after iteration 9800: 0.187500\n",
            "Cost after iteration 9900: 73.644496\n",
            "Accuracy after iteration 9900: 0.187500\n",
            "accuracy:  0.1875\n",
            "cost:  73.64411479834541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAD4CAYAAAAjKGdbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvCWpQIa3JBXI8JakAhneklQgw1uSCvR/oFGevxdvTjgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0559e9a4a8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Zwut_stWllMu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        },
        "cellView": "both"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "test_2 = MLP_train(m=20000)\n",
        "test_2.fit(epochs = 10000, mini_batch_size = 128, hidden_layers = [], keep_prob =.5, learning_rate = .0000075, beta = .8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iq92ijJ5N4wI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "collapsed": true,
        "cellView": "form",
        "outputId": "0564f960-c2fb-413d-ecc7-a1548d3b0261",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525156083704,
          "user_tz": -600,
          "elapsed": 682,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class MLP_train_random():\n",
        "    \"\"\"\n",
        "    hidden layers - a python list\n",
        "    \"\"\"\n",
        "    def __init__(self, m, num_features, yn):\n",
        "        \n",
        "        \n",
        "          \n",
        "        self.m = m\n",
        "        self.num_features = num_features\n",
        "        self.yn = yn\n",
        "        np.random.seed(1)\n",
        "        self.X_train = np.random.rand(m, num_features)\n",
        "        self.labels_train = np.random.randint(yn, size = (m))\n",
        "    def fit(self, mini_batch_size = 1, beta = .8, keep_prob = 1, learning_rate = .005, epochs = 100, hidden_layers = None):\n",
        "        \n",
        "        m = self.m\n",
        "        num_features = self.num_features\n",
        "        yn = self.yn\n",
        "        \n",
        "        if mini_batch_size > m: \n",
        "          raise ValueError('mini_batch_size too big')\n",
        "         \n",
        "        if hidden_layers == None:\n",
        "            layers_number = [num_features, yn]\n",
        "        else:\n",
        "            layers_number = [num_features] + hidden_layers + [yn]\n",
        "          \n",
        "        case = MLP(layers_number, keep_prob, beta, mini_batch_size)\n",
        "        cost, accuracy = case.fit(self.X_train, self.labels_train, learning_rate, epochs)\n",
        "\n",
        "        %matplotlib inline\n",
        "        \n",
        "        pl.plot(cost)\n",
        "        pl.plot(accuracy)\n",
        "  "
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: 16.094379\n",
            "Accuracy after iteration 0: 0.000000\n",
            "Cost after iteration 100: 15.479602\n",
            "Accuracy after iteration 100: 1.000000\n",
            "accuracy:  1.0\n",
            "cost:  14.84518929283325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-89a49cc5ff06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ]
    }
  ]
}