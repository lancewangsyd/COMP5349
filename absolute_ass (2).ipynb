{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "absolute_ass.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ln9HR5xfWblD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###code source\n",
        "https://blog.csdn.net/clayanddev/article/details/53955544\n"
      ]
    },
    {
      "metadata": {
        "id": "JG2jdfBjUrnY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets, linear_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class NNModel:\n",
        "    Ws = [] # params W of the whole network\n",
        "    bs = [] # params b of the whole network\n",
        "    layers = [] # number of nodes in each layer\n",
        "    epsilon = 0.01 # learning rate epsilon\n",
        "\n",
        "    def __init__(self, layers, epsilon, activation = \"relu\"):\n",
        "        self.layers = layers\n",
        "        self.epsilon = epsilon\n",
        "        self.process_name = None\n",
        "        self.activation_name = activation\n",
        "        \n",
        "        self.activation=Activation(activation).f\n",
        "        self.activation_deriv=Activation(activation).f_deriv\n",
        "        \n",
        "        self.init_params()\n",
        "\n",
        "    # Initialize the parameters (W and b) to random values. We need to learn these.\n",
        "    def init_params(self):\n",
        "        np.random.seed(0)\n",
        "        layers = self.layers\n",
        "        hidden_layer_num = len(layers) - 1\n",
        "        Ws = [1] * hidden_layer_num\n",
        "        bs = [1] * hidden_layer_num\n",
        "        for i in range(0, hidden_layer_num):\n",
        "            if self.activation_name == \"relu\":\n",
        "              Ws[i] = np.random.randn(layers[i], layers[i + 1]) / np.sqrt(layers[i]/2) \n",
        "            elif self.activation_name == \"tanh\":\n",
        "              Ws[i] = np.random.randn(layers[i], layers[i + 1]) / np.sqrt(layers[i]/2)\n",
        "\n",
        "            bs[i] = np.zeros((1, layers[i + 1]))\n",
        "        self.Ws = Ws\n",
        "        self.bs = bs\n",
        "\n",
        "    # This function learns parameters for the neural network from training dataset\n",
        "    # - num_passes: Number of passes through the training data for gradient descent\n",
        "    # - print_loss: If True, print the loss every 1000 iterations\n",
        "    def train(self, X, y, num_passes=20000, keep_prob = 1, beta = 0, reg_lambda = 0.01, process = None, print_loss=False, X_test = None, y_test = None):\n",
        "        num_examples = len(X)\n",
        "        expected_output = y\n",
        "        losses = []\n",
        "        accuracys = []\n",
        "        \n",
        "        losses_test = []\n",
        "        accuracys_test = []\n",
        "        \n",
        "        self.process_name = process\n",
        "        \n",
        "        if (X_test is not None) and (y_test is not None):         \n",
        "            test = True\n",
        "        else:\n",
        "            test = False\n",
        "            \n",
        "        if process is not None:\n",
        "            process_function = Process(process).p\n",
        "            \n",
        "            X = process_function(X)\n",
        "        \n",
        "\n",
        "        # Gradient descent. For each batch...\n",
        "        for i in range(0, num_passes+1):\n",
        "\n",
        "            # Forward propagation\n",
        "            a_output = self.forward(X, keep_prob)\n",
        "\n",
        "            # Backpropagation\n",
        "            v_dWs, v_dbs = self.backward(X, expected_output, a_output, keep_prob, beta)\n",
        "\n",
        "            # Update parameters of the model\n",
        "            self.update_model_params(v_dWs, v_dbs, num_examples, reg_lambda)\n",
        "            \n",
        "            #record cost\n",
        "            loss = self.calculate_loss(X, expected_output, 1, reg_lambda)\n",
        "            \n",
        "            losses.append(loss)\n",
        "            \n",
        "            accuracy = 1-(np.count_nonzero(self.predict(X) - y))/len(X)\n",
        "            \n",
        "            \n",
        "            accuracys.append(accuracy)\n",
        "            \n",
        "            # test case\n",
        "            if test:   \n",
        "                if process is not None:\n",
        "                    X_test = process_function(X_test)\n",
        "                loss_test = self.calculate_loss(X_test, y_test, 1, reg_lambda)\n",
        "                losses_test.append(loss_test)\n",
        "                accuracy_test = 1-(np.count_nonzero(self.predict(X_test) - y_test))/len(X_test)\n",
        "                accuracys_test.append(accuracy_test)\n",
        "            \n",
        "            # Optionally print the loss.\n",
        "            # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n",
        "            if print_loss and i % 100 == 0:\n",
        "                print(\"Loss after iteration %i: %f\" % (i, loss))\n",
        "        if test:\n",
        "            return losses, accuracys, losses_test, accuracys_test\n",
        "        else:\n",
        "            return losses, accuracys\n",
        "\n",
        "    # Helper function to evaluate the total loss on the dataset\n",
        "    def calculate_loss(self, X, expected_output, keep_prob, reg_lambda):\n",
        "        num_examples = len(X)  # training set size\n",
        "\n",
        "        # Forward propagation to calculate our predictions\n",
        "        a_output = self.forward(X, keep_prob)\n",
        "        probs = a_output[-1]\n",
        "\n",
        "        # Calculating the loss\n",
        "        corect_logprobs = -np.log(probs[range(num_examples), expected_output])\n",
        "        data_loss = np.sum(corect_logprobs)\n",
        "        # Add regulatization term to loss (optional)\n",
        "        for W in self.Ws:\n",
        "            data_loss += reg_lambda / 2 * np.sum(np.square(W))\n",
        " \n",
        "        return 1. / num_examples * data_loss\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, X, keep_prob):\n",
        "        Ws = self.Ws\n",
        "        bs = self.bs\n",
        "        hidden_layer_num = len(Ws)\n",
        "        a_output = [1] * hidden_layer_num\n",
        "        current_input = X\n",
        "\n",
        "        for i in range(0, hidden_layer_num - 1):\n",
        "            w_current = Ws[i]\n",
        "            b_current = bs[i]\n",
        "            z_current = current_input.dot(w_current) + b_current\n",
        "            a_current = self.activation(z_current)\n",
        "            \n",
        "            d = np.random.rand(a_current.shape[0],a_current.shape[1]) < keep_prob\n",
        "            \n",
        "            a_current = np.multiply(d, a_current)\n",
        "            \n",
        "            a_current /= keep_prob\n",
        "            \n",
        "            a_output[i] = a_current\n",
        "            current_input = a_current\n",
        "            \n",
        "        keep_prob = 1\n",
        "\n",
        "        #output layer(softmax)\n",
        "        z_current = current_input.dot(Ws[hidden_layer_num - 1]) + bs[hidden_layer_num - 1]\n",
        "        a_current = softmax(z_current)\n",
        "        a_output[hidden_layer_num - 1] = a_current\n",
        "        return a_output\n",
        "\n",
        "    \n",
        "    # Predict the result of classification of input x\n",
        "    def predict(self, x):\n",
        "        a_output = self.forward(x, keep_prob = 1)\n",
        "        return np.argmax(a_output[-1], axis=1)\n",
        "\n",
        "    # Backpropagation\n",
        "    def backward(self, X, expected_output, a_output, keep_prob, beta):\n",
        "        Ws = self.Ws\n",
        "        bs = self.bs\n",
        "        hidden_layer_num = len(Ws)\n",
        "        \n",
        "        num_examples = len(X)\n",
        "        ds = [1] * hidden_layer_num\n",
        "        \n",
        "\n",
        "        # output layer\n",
        "        d_current = a_output[hidden_layer_num - 1]\n",
        "        \n",
        "\n",
        "        d_current[range(num_examples), expected_output] -= 1\n",
        "        \n",
        "\n",
        "        ds[hidden_layer_num - 1] = d_current\n",
        "        \n",
        "\n",
        "        #other hidden layer\n",
        "        for l in range(hidden_layer_num - 2, -1, -1):\n",
        "            w_current = Ws[l + 1]\n",
        "            a_current = a_output[l]\n",
        "            d_current = np.dot(d_current, w_current.T) * (self.activation_deriv(a_current))\n",
        "            ds[l] = d_current / keep_prob\n",
        "\n",
        "        #calc dW && db\n",
        "        dWs = [1] * hidden_layer_num\n",
        "        dbs = [1] * hidden_layer_num\n",
        "        \n",
        "        v_dWs = [0] * hidden_layer_num\n",
        "        v_dbs = [0] * hidden_layer_num\n",
        "        \n",
        "        \n",
        "        a_last = X\n",
        "        num_output = len(X)\n",
        "        for l in range(0, hidden_layer_num):\n",
        "            d_current = ds[l]\n",
        "            dWs[l] = np.dot(a_last.T, d_current)\n",
        "            dbs[l] = np.sum(d_current, axis=0, keepdims=True)\n",
        "            \n",
        "            v_dWs[l] = beta * v_dWs[l] + (1-beta) * dWs[l]\n",
        "            v_dbs[l] = beta * v_dbs[l] + (1-beta) * dbs[l]\n",
        "            \n",
        "            a_last = a_output[l]\n",
        "        return v_dWs, v_dbs\n",
        "\n",
        "    # Update the params (Ws and bs) of the netword during Backpropagation\n",
        "    def update_model_params(self, v_dWs, v_dbs, num_examples, reg_lambda):\n",
        "        Ws = self.Ws\n",
        "        bs = self.bs\n",
        "        hidden_layer_num = len(Ws)\n",
        "        for l in range(0, hidden_layer_num):\n",
        "            Ws[l] = Ws[l] - self.epsilon * (v_dWs[l] + reg_lambda* Ws[l])/ num_examples\n",
        "            bs[l] = bs[l] - self.epsilon * (v_dbs[l])/ num_examples\n",
        "\n",
        "        self.Ws = Ws\n",
        "        self.bs = bs\n",
        "\n",
        "def softmax(x):\n",
        "        exp_scores = np.exp(x)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return probs\n",
        "      \n",
        "class Activation():\n",
        "   \n",
        "    def __tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def __tanh_deriv(self, a):\n",
        "        return 1-a**2\n",
        "\n",
        "    def __relu(self, x):       \n",
        "        return np.maximum(0,x)\n",
        "\n",
        "    def __relu_deriv(self, a):\n",
        "        a[a>0]=1\n",
        "        a[a<=0]=0\n",
        "        return a  \n",
        "      \n",
        "    def __init__(self,activation='relu'):\n",
        "        if activation == 'relu':\n",
        "            self.f = self.__relu\n",
        "            self.f_deriv = self.__relu_deriv\n",
        "        elif activation == 'tanh':\n",
        "            self.f = self.__tanh\n",
        "            self.f_deriv = self.__tanh_deriv\n",
        "\n",
        "class Process():\n",
        "    def __z_score(self, x):\n",
        "        x = np.array(x).astype(float)\n",
        "        xr = np.rollaxis(x, axis=1)\n",
        "        xr -= np.mean(x, axis=1)\n",
        "        xr /= np.std(x, axis=1)\n",
        "        return x\n",
        "\n",
        "    def __norm(self, x):\n",
        "        x = np.array(x).astype(float)\n",
        "        xr = np.rollaxis(x, axis=1)\n",
        "        x_mean = np.mean(x, axis = 1)\n",
        "        x_max = np.max(x, axis = 1)\n",
        "        x_min = np.min(x, axis = 1)\n",
        "        xr -= x_mean\n",
        "        xr /=(x_max-x_min)\n",
        "        return x\n",
        "      \n",
        "    def __init__(self, process='norm'):\n",
        "        if process == 'norm':\n",
        "            self.p = self.__norm\n",
        "        elif process == 'z_score':\n",
        "            self.p = self.__z_score\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-hlZtm-EWoCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###import data"
      ]
    },
    {
      "metadata": {
        "id": "qFPUhE5DZZ8M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "X_test= X[-10000:]\n",
        "y_test= labels[-10000:]\n",
        "X_train = X[:5000]\n",
        "y_train = labels[:5000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fdTIoMXXLuRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##current optimal config for tanh 50000"
      ]
    },
    {
      "metadata": {
        "id": "-nIg_6tgSdDs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Config_tanh:\n",
        "    # Gradient descent parameters (I picked these by hand)\n",
        "    epsilon = 0.1  # learning rate for gradient descent\n",
        "    layers = [128,20,40,10] # number of nodes in each layer\n",
        "    activation  = \"tanh\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FajZVApLkpBa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## current optimal config for relu 50000"
      ]
    },
    {
      "metadata": {
        "id": "uFQ5ikIcLtgf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class Config_relu:\n",
        "    # Gradient descent parameters (I picked these by hand)\n",
        "    epsilon = 0.075  # learning rate for gradient descent\n",
        "    layers = [128,20,40,10] # number of nodes in each layer\n",
        "    activation = \"relu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jsZQGbXtUEXb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def plot_result():\n",
        "    plt.title('losses_train, accuracys_train, losses_test, accuracys_test')\n",
        "    \n",
        "    plt.plot(losses, label = 'losses = %f' %(losses[-1]))\n",
        "      \n",
        "    \n",
        "    plt.plot(losses_test, label = 'losses_test = %f' %(losses_test[-1]))\n",
        "    \n",
        "    plt.plot(accuracys, label = 'accuracys = %f' %(accuracys[-1]))\n",
        "    \n",
        "    plt.plot(accuracys_test, label = 'accuracys_test = %f' %(accuracys_test[-1]))\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.xlabel('epochs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "is1hrZcrk2CT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## main train process"
      ]
    },
    {
      "metadata": {
        "id": "oyOAiji5i1fa",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6815b25f-0d5d-4ba3-fc74-7bffe0a9de58",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525843966982,
          "user_tz": -600,
          "elapsed": 11725,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "losses, accuracys, losses_test, accuracys_test = [],[],[],[]\n",
        "M1 = NNModel(Config_tanh.layers, Config_tanh.epsilon)\n",
        "\"\"\", losses_test, accuracys_test\"\"\"\n",
        "losses, accuracys  = (\n",
        "    M1.train(X_train, y_train, num_passes=200, keep_prob = .8, beta = 0.8, reg_lambda = 0.01, process = \"z_score\", \n",
        "                print_loss=True, X_test = None, y_test = None))\n",
        "\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "#plot_result()\n"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0: 2.976268\n",
            "Loss after iteration 100: 1.481803\n",
            "Loss after iteration 200: 1.121479\n",
            "--- 10.890503168106079 seconds ---\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mPMaHDCDWWH2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### accuracy "
      ]
    },
    {
      "metadata": {
        "id": "JQ03XJyJk82X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train accuracy"
      ]
    },
    {
      "metadata": {
        "id": "kpedqfm0GjwX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ec6b35a-b61a-466f-b36a-4fb8ed27136e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525761109303,
          "user_tz": -600,
          "elapsed": 1018,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "1-(np.count_nonzero(model.predict(X_train) - labels_train))/len(X_train)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5498000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "athSdn55k_Cn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "test accuracy"
      ]
    },
    {
      "metadata": {
        "id": "tJ-mkAqbZi5k",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51e51291-71cc-44d4-a3cb-c89a371adade",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525761106935,
          "user_tz": -600,
          "elapsed": 1080,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "1-(np.count_nonzero(model.predict(X_predict) - labels_predict))/len(X_predict)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "metadata": {
        "id": "Azz89_eyMVzs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## check the impact of activation"
      ]
    },
    {
      "metadata": {
        "id": "OD1xMoGPMaBM",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uue2cM0hlXRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## check the impact of beta in sgd momentum"
      ]
    },
    {
      "metadata": {
        "id": "ns3Vv49glWm9",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "betas = np.linspace(0, 1, 20, endpoint=True)\n",
        "\n",
        "losses_prob = []\n",
        "accuracys_pred_prob= []\n",
        "for k in betas:\n",
        "    \n",
        "    model = NNModel(Config.layers, Config.epsilon)\n",
        "    \n",
        "    losses_train, accuracys_train = model.train(X_train, labels_train, num_passes=500, keep_prob = 1, beta = k, print_loss=False)\n",
        "    \n",
        "    print(\"Train_Loss for keep_prob = %f: %f\" % (k, losses_train[-1]))\n",
        "    \n",
        "    losses_prob.append(losses_train[-1])\n",
        "    \n",
        "    accuracys = 1- (np.count_nonzero(model.predict(X_predict) - labels_predict))/len(X_predict)\n",
        "    \n",
        "    print(\"Predict_accuracy for keep_prob = %f: %f\" % (k, accuracys))\n",
        "    \n",
        "    accuracys_pred_prob.append(accuracys)\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YdPIPn6pdtIp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##check the impact of keep_prob"
      ]
    },
    {
      "metadata": {
        "id": "zlheoCXJd2E-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "43a547c3-b5c3-4af4-ed4b-904fc666c9e8",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525777765723,
          "user_tz": -600,
          "elapsed": 810,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "keep_probs = np.linspace(0.1, 1, 19, endpoint=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3267e1b69f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeep_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "Yp9kClKEfECx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "9783d699-8764-4b29-ced9-5b58bdfc2190",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525763012597,
          "user_tz": -600,
          "elapsed": 542779,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "losses_prob = []\n",
        "accuracys_pred_prob= []\n",
        "for k in keep_probs:\n",
        "    \n",
        "    model = NNModel(Config.layers, Config.epsilon)\n",
        "    \n",
        "    losses_train, accuracys_train = model.train(X_train, labels_train, num_passes=5000, keep_prob =k, beta = 0.8, print_loss=False)\n",
        "    \n",
        "    print(\"Train_Loss for keep_prob = %f: %f\" % (k, losses_train[-1]))\n",
        "    \n",
        "    losses_prob.append(losses_train[-1])\n",
        "    \n",
        "    accuracys = 1- (np.count_nonzero(model.predict(X_predict) - labels_predict))/len(X_predict)\n",
        "    \n",
        "    print(\"Predict_accuracy for keep_prob = %f: %f\" % (k, accuracys))\n",
        "    \n",
        "    accuracys_pred_prob.append(accuracys)\n",
        "    \n",
        "plt.title('Train loss/ test accuracy for different beta')\n",
        "plt.plot(betas, losses_prob, label = 'trian_loss')\n",
        "plt.plot(betas, accuracys_pred_prob, label = 'test_accuracy')\n",
        "plt.xlabel('keep_probs')    \n",
        "plt.legend()   \n",
        "    "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train_Loss for keep_prob = 0.100000: 2.005353\n",
            "Predict_accuracy for keep_prob = 0.100000: 0.196900\n",
            "Train_Loss for keep_prob = 0.150000: 1.807452\n",
            "Predict_accuracy for keep_prob = 0.150000: 0.316600\n",
            "Train_Loss for keep_prob = 0.200000: 1.680609\n",
            "Predict_accuracy for keep_prob = 0.200000: 0.460800\n",
            "Train_Loss for keep_prob = 0.250000: 1.573054\n",
            "Predict_accuracy for keep_prob = 0.250000: 0.436500\n",
            "Train_Loss for keep_prob = 0.300000: 1.466566\n",
            "Predict_accuracy for keep_prob = 0.300000: 0.463900\n",
            "Train_Loss for keep_prob = 0.350000: 1.343462\n",
            "Predict_accuracy for keep_prob = 0.350000: 0.580800\n",
            "Train_Loss for keep_prob = 0.400000: 1.269820\n",
            "Predict_accuracy for keep_prob = 0.400000: 0.632200\n",
            "Train_Loss for keep_prob = 0.450000: 1.174522\n",
            "Predict_accuracy for keep_prob = 0.450000: 0.662900\n",
            "Train_Loss for keep_prob = 0.500000: 1.098272\n",
            "Predict_accuracy for keep_prob = 0.500000: 0.674000\n",
            "Train_Loss for keep_prob = 0.550000: 1.018894\n",
            "Predict_accuracy for keep_prob = 0.550000: 0.685200\n",
            "Train_Loss for keep_prob = 0.600000: 0.942082\n",
            "Predict_accuracy for keep_prob = 0.600000: 0.722900\n",
            "Train_Loss for keep_prob = 0.650000: 0.867339\n",
            "Predict_accuracy for keep_prob = 0.650000: 0.739300\n",
            "Train_Loss for keep_prob = 0.700000: 0.787465\n",
            "Predict_accuracy for keep_prob = 0.700000: 0.747600\n",
            "Train_Loss for keep_prob = 0.750000: 0.674377\n",
            "Predict_accuracy for keep_prob = 0.750000: 0.762200\n",
            "Train_Loss for keep_prob = 0.800000: 0.582894\n",
            "Predict_accuracy for keep_prob = 0.800000: 0.785300\n",
            "Train_Loss for keep_prob = 0.850000: 0.511508\n",
            "Predict_accuracy for keep_prob = 0.850000: 0.795200\n",
            "Train_Loss for keep_prob = 0.900000: 0.454862\n",
            "Predict_accuracy for keep_prob = 0.900000: 0.789100\n",
            "Train_Loss for keep_prob = 0.950000: 0.368846\n",
            "Predict_accuracy for keep_prob = 0.950000: 0.800900\n",
            "Train_Loss for keep_prob = 1.000000: 0.246772\n",
            "Predict_accuracy for keep_prob = 1.000000: 0.801400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xTLqL1agkFPd",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "43cbd0af-457f-4bd9-a0ab-13d49c95e909",
        "executionInfo": {
          "status": "error",
          "timestamp": 1525777713500,
          "user_tz": -600,
          "elapsed": 1204,
          "user": {
            "displayName": "Lan",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "106352409797232494979"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "plt.title('Train loss/ test accuracy for different keep_prob')\n",
        "plt.plot(keep_probs, losses_prob, label = 'trian_loss')\n",
        "plt.plot(keep_probs, accuracys_pred_prob, label = 'test_accuracy')\n",
        "plt.xlabel('keep_probs')\n",
        "plt.legend()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-58a35124424c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train loss/ test accuracy for different keep_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'trian_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracys_pred_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test_accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'keep_probs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "VPPTiFWGYiTt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### upload from google drive"
      ]
    },
    {
      "metadata": {
        "id": "pF7WWWmbhxjh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "alcQFStrleBR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "\n",
        "from google.colab import auth \n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dkaf5a1Dlslx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Generate creds for the Drive FUSE library.\n",
        "\n",
        "from oauth2client.client import GoogleCredentials \n",
        "creds = GoogleCredentials.get_application_default()\n",
        "\n",
        "import getpass\n",
        "\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "\n",
        "vcode = getpass.getpass()\n",
        "\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T7s4hQiHlzDT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Create a directory and mount Google Drive using that directory.\n",
        "\n",
        "!mkdir -p drive\n",
        "\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uculMfpFmjOX",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "with h5py.File('drive/Colab Notebooks/deep_data_ass_1/train_128.h5','r') as H:\n",
        "    X = np.copy(H['data'])\n",
        "\n",
        "with h5py.File('drive/Colab Notebooks/deep_data_ass_1/train_label.h5','r') as H:\n",
        "    labels = np.copy(H['label'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}